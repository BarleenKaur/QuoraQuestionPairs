{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Barly/Anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import types\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from sklearn import svm\n",
    "import codecs, csv, sys\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB                                                                \n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the csv containg embedding for test and train embedding data\n",
    "df_train = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Word2Vec/train_word2vec.csv\")\n",
    "df_test = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Word2Vec/test_word2vec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenating the embedding( sum and average) for question1 and question 2 and normalizing before passing it to classifiers\n",
    "start_time = time.time()\n",
    "\n",
    "train_embedding1_sum = map(np.fromstring, df_train['embedding_1_sum'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "\n",
    "train_embedding2_sum = map(np.fromstring, df_train['embedding_2_sum'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "\n",
    "train_embedding1_avg = map(np.fromstring, df_train['embedding_1_avg'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "\n",
    "train_embedding2_avg = map(np.fromstring, df_train['embedding_2_avg'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "X_label = np.array(df_train['is_duplicate'].tolist())\n",
    "X_sum = np.concatenate((np.array(train_embedding1_sum), np.array(train_embedding2_sum)), axis=1)\n",
    "normalize_sum = Normalizer().fit(X_sum)\n",
    "X_sum = normalize_sum.transform(X_sum)\n",
    "\n",
    "X_avg = np.concatenate((np.array(train_embedding1_avg), np.array(train_embedding2_avg)), axis=1)\n",
    "normalize_avg = Normalizer().fit(X_avg)\n",
    "X_avg = normalize_avg.transform(X_avg)\n",
    "\n",
    "test_embedding1_sum = map(np.fromstring, df_test['embedding_1_sum'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "\n",
    "test_embedding2_sum = map(np.fromstring, df_test['embedding_2_sum'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "\n",
    "test_embedding1_avg = map(np.fromstring, df_test['embedding_1_avg'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "\n",
    "test_embedding2_avg = map(np.fromstring, df_test['embedding_2_avg'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "Y_label = np.array(df_test['is_duplicate'].tolist())\n",
    "Y_sum = np.concatenate((np.array(test_embedding1_sum), np.array(test_embedding2_sum)), axis=1) # concatenating the embedding for question 1 and question2\n",
    "Y_sum = normalize_sum.transform(Y_sum)\n",
    "\n",
    "Y_avg = np.concatenate((np.array(test_embedding1_sum), np.array(test_embedding2_sum)), axis=1)\n",
    "Y_avg = normalize_avg.transform(Y_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting is_duplicate value by passing embedding into shallow classifiers\n",
    "def naive_bayes(x_train_matrix, x_test_matrix, y_train, y_test):    #NAIVE BAYERS MODEL                                                                \n",
    "    nb = MultinomialNB(alpha=2.8)\n",
    "    nb.fit(x_train_matrix, y_train)                                 #train the model\n",
    "    y_pred = nb.predict(x_test_matrix)                              #make predictions for X_test\n",
    "    print (\"Naive Bayes Model score: \" + str(accuracy_score(y_test, y_pred)))         \n",
    "    print (\"Naive Bayes Model confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred))\n",
    "    y_predicted_proba = nb.predict_proba(x_test_matrix)\n",
    "    print log_loss(y_test, y_predicted_proba)\n",
    "    return y_pred\n",
    "\n",
    "def logistic_regression(x_train_matrix, x_test_matrix, y_train, y_test): \n",
    "    logReg = LogisticRegressionCV(Cs=[0.01, 0.1, 1, 10, 100], cv=5, solver='saga', n_jobs= -1 )\n",
    "    logReg.fit(x_train_matrix, y_train)\n",
    "    y_pred_log = logReg.predict(x_test_matrix)\n",
    "    print (\"Logistic Regression score: \"+ str(accuracy_score(y_test, y_pred_log)))\n",
    "    print (\"Logistic Regression confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred_log))\n",
    "    y_predicted_proba = logReg.predict_proba(x_test_matrix)\n",
    "    print log_loss(y_test, y_predicted_proba)\n",
    "    return y_pred_log\n",
    "\n",
    "def linear_svm(x_train_matrix, x_test_matrix, y_train, y_test):\n",
    "    \n",
    "    param_grid = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']}]\n",
    "    svc = svm.SVC()\n",
    "    svm1 = GridSearchCV(estimator=svc, param_grid=param_grid, n_jobs= -1, cv = 5) \n",
    "    #svm1 = LinearSVC(C=1)\n",
    "    svm1.fit(x_train_matrix, y_train)\n",
    "    y_pred_svc = svm1.predict(x_test_matrix)\n",
    "    print (\"SVC score: \" + str(accuracy_score(y_test, y_pred_svc)))\n",
    "    print (\"SVC confusionmatrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred_svc))\n",
    "    return y_pred_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- start -> preprocessing:1.61976790428 seconds ---\n",
      "\n",
      "\n",
      " Naive bayes result on word_embedding_sum: \n",
      "Naive Bayes Model score: 0.49\n",
      "Naive Bayes Model confusion matrix:\n",
      "[[210  40]\n",
      " [215  35]]\n",
      "0.69650563621\n",
      "\n",
      "\n",
      " Naive bayes result on word_embedding_avg: \n",
      "Naive Bayes Model score: 0.516\n",
      "Naive Bayes Model confusion matrix:\n",
      "[[220  30]\n",
      " [212  38]]\n",
      "0.693893849147\n",
      "--- preprocessing -> naive bayes :0.0313489437103 seconds ---\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      " Logistic Regression result on word_embedding_sum: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Barly/Anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score: 0.484\n",
      "Logistic Regression confusion matrix:\n",
      "[[206  44]\n",
      " [214  36]]\n",
      "0.693310724\n",
      "\n",
      "\n",
      " Logistic Regression result on word_embedding_avg: \n",
      "Logistic Regression score: 0.5\n",
      "Logistic Regression confusion matrix:\n",
      "[[212  38]\n",
      " [212  38]]\n",
      "0.693560805331\n",
      "--- naive bayes -> logistic regression :34.3417680264 seconds ---\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      " Linear SVM result on word_embedding_sum: \n",
      "SVC score: 0.482\n",
      "SVC confusionmatrix:\n",
      "[[213  37]\n",
      " [222  28]]\n",
      "\n",
      "\n",
      " Linear SVM result on word_embedding_avg: \n",
      "SVC score: 0.512\n",
      "SVC confusionmatrix:\n",
      "[[217  33]\n",
      " [211  39]]\n",
      "--- logistic regression -> linear svm :675.202017069 seconds ---\n",
      "--- TOTAL TIME: 711.195041895 seconds ---\n"
     ]
    }
   ],
   "source": [
    "preprocessing_time = time.time()\n",
    "print(\"--- start -> preprocessing:%s seconds ---\" % (preprocessing_time - start_time))\n",
    "\n",
    "\n",
    "print (\"\\n\\n Naive bayes result on word_embedding_sum: \")\n",
    "y_pred_nb_sum = naive_bayes(X_sum, Y_sum, X_label, Y_label)\n",
    "print (\"\\n\\n Naive bayes result on word_embedding_avg: \")\n",
    "y_pred_nb_avg = naive_bayes(X_avg, Y_avg, X_label, Y_label)\n",
    "naive_bayes_time = time.time() \n",
    "print(\"--- preprocessing -> naive bayes :%s seconds ---\" % (naive_bayes_time - preprocessing_time))\n",
    "\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "print (\"\\n\\n Logistic Regression result on word_embedding_sum: \")\n",
    "y_pred_lr_sum = logistic_regression(X_sum, Y_sum, X_label, Y_label)\n",
    "print (\"\\n\\n Logistic Regression result on word_embedding_avg: \")\n",
    "y_pred_lr_avg = logistic_regression(X_avg, Y_avg, X_label, Y_label)\n",
    "logistic_regression_time = time.time()\n",
    "print(\"--- naive bayes -> logistic regression :%s seconds ---\" % (logistic_regression_time - naive_bayes_time))\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "print (\"\\n\\n Linear SVM result on word_embedding_sum: \")\n",
    "y_pred_svm_sum = linear_svm(X_sum, Y_sum, X_label, Y_label)\n",
    "print (\"\\n\\n Linear SVM result on word_embedding_avg: \")\n",
    "y_pred_svm_avg = linear_svm(X_avg, Y_avg, X_label, Y_label)\n",
    "linear_svm_time = time.time()\n",
    "print(\"--- logistic regression -> linear svm :%s seconds ---\" % (linear_svm_time - logistic_regression_time))\n",
    "print(\"--- TOTAL TIME: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_nb_sum = y_pred_nb_sum.tolist()\n",
    "y_pred_nb_avg = y_pred_nb_avg.tolist()\n",
    "y_pred_lr_sum = y_pred_lr_sum.tolist()\n",
    "y_pred_lr_avg = y_pred_lr_avg.tolist()\n",
    "y_pred_svm_sum = y_pred_svm_sum.tolist()\n",
    "y_pred_svm_avg = y_pred_svm_avg.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['nb_sum_duplicate'] = y_pred_nb_sum\n",
    "df_test['nb_avg_duplicate'] = y_pred_nb_avg\n",
    "df_test['lr_sum_duplicate'] = y_pred_lr_sum\n",
    "df_test['lr_avg_duplicate'] = y_pred_lr_avg\n",
    "df_test['svm_sum_duplicate'] = y_pred_svm_sum\n",
    "df_test['svm_avg_duplicate'] = y_pred_svm_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.to_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type3/Results/test_word2vec_classifier.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparing the cosine similarity between word2vec vectors on different thresholds\n",
    "df_new = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type3/Results/test_word2vec_classifier.csv\")\n",
    "\n",
    "def conv(elem):\n",
    "    a = re.compile(r\"[-+]?\\d*\\.\\d+|\\d+\")\n",
    "    return re.findall(a, elem)[0]\n",
    "\n",
    "threshold_list = [0.6, 0.7,0.78, 0.8]\n",
    "df_cos_test = df_new\n",
    "df_cos_test['cos_avg'] = map(conv, df_cos_test['cos_avg'])\n",
    "df_cos_test['cos_sum'] = map(conv, df_cos_test['cos_sum'])\n",
    "df_cos_test[['cos_avg','cos_sum']] = df_cos_test[['cos_avg','cos_sum']].apply(pd.to_numeric)\n",
    "\n",
    "for threshold in threshold_list:\n",
    "    \n",
    "    print \"Threshold: \", threshold\n",
    "    \n",
    "    df_cos_test['thre_sum_'+str(threshold)] = np.where(df_cos_test['cos_sum']>=threshold, 1, 0)\n",
    "    df_cos_test['thre_avg_'+str(threshold)] = np.where(df_cos_test['cos_avg']>=threshold, 1, 0)\n",
    "\n",
    "    print confusion_matrix(df_cos_test['is_duplicate'], df_cos_test['thre_sum_'+str(threshold)]\\\n",
    "                                      , labels=target_names).ravel()\n",
    "    print classification_report(df_cos_test['is_duplicate'], df_cos_test['thre_sum_'+str(threshold)])\n",
    "    print accuracy_score(df_cos_test['is_duplicate'], df_cos_test['thre_sum_'+str(threshold)])\n",
    "    print confusion_matrix(df_cos_test['is_duplicate'], df_cos_test['thre_avg_'+str(threshold)]\\\n",
    "                                      , labels=target_names).ravel()\n",
    "    \n",
    "    print classification_report(df_cos_test['is_duplicate'], df_cos_test['thre_avg_'+str(threshold)])\n",
    "\n",
    "    print \"Accuracy: \" ,accuracy_score(df_cos_test['is_duplicate'], df_cos_test['thre_avg_'+str(threshold)])\n",
    "    print \"\\n*******************\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
