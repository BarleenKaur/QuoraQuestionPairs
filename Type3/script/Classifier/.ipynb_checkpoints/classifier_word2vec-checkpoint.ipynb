{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Barly/Anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import types\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "from autocorrect import spell\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import codecs, csv, sys\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB                                                                \n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type3/data/doc2Vec/train_doc2vec.csv\")\n",
    "df_test = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type3/data/doc2Vec/test_doc2vec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'embedding_1_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ddf2dd0856a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_embedding1_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding_1_sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                      \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                       \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_embedding2_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding_2_sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                      \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                       \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Barly/Anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2060\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Barly/Anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Barly/Anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Barly/Anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Barly/Anaconda2/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2393\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2395\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5239)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20405)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20359)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'embedding_1_sum'"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_embedding1_sum = map(np.fromstring, df_train['embedding_1_sum'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "\n",
    "train_embedding2_sum = map(np.fromstring, df_train['embedding_2_sum'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "\n",
    "train_embedding1_avg = map(np.fromstring, df_train['embedding_1_avg'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "\n",
    "train_embedding2_avg = map(np.fromstring, df_train['embedding_2_avg'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "X_label = np.array(df_train['is_duplicate'].tolist())\n",
    "X_sum = np.concatenate((np.array(train_embedding1_sum), np.array(train_embedding2_sum)), axis=1)\n",
    "X_avg = np.concatenate((np.array(train_embedding1_avg), np.array(train_embedding2_avg)), axis=1)\n",
    "\n",
    "test_embedding1_sum = map(np.fromstring, df_test['embedding_1_sum'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "\n",
    "test_embedding2_sum = map(np.fromstring, df_test['embedding_2_sum'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "\n",
    "test_embedding1_avg = map(np.fromstring, df_test['embedding_1_avg'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "\n",
    "test_embedding2_avg = map(np.fromstring, df_test['embedding_2_avg'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "Y_label = np.array(df_test['is_duplicate'].tolist())\n",
    "Y_sum = np.concatenate((np.array(test_embedding1_sum), np.array(test_embedding2_sum)), axis=1)\n",
    "Y_avg = np.concatenate((np.array(test_embedding1_sum), np.array(test_embedding2_sum)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes(x_train_matrix, x_test_matrix, y_train, y_test):    #NAIVE BAYERS MODEL\n",
    "                                                                    \n",
    "    nb = GaussianNB()\n",
    "    nb.fit(x_train_matrix, x_test_matrix)                                 #train the model\n",
    "    y_pred = nb.predict(y_train)                              #make predictions for X_test\n",
    "    print (\"Naive Bayes Model score: \" + str(accuracy_score(y_test, y_pred)))         \n",
    "    print (\"Naive Bayes Model confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred))\n",
    "    #y_predicted_proba = nb.predict_proba(y_train)\n",
    "   # print log_loss(y_test, y_predicted_proba)\n",
    "    print classification_report(y_test, y_pred)\n",
    "    \n",
    "def logistic_regression(x_train_matrix, x_test_matrix, y_train, y_test): \n",
    "    logReg = LogisticRegression(penalty=\"l2\")\n",
    "    logReg.fit(x_train_matrix, x_test_matrix)\n",
    "    y_pred_log = logReg.predict(y_train)\n",
    "    print (\"Logistic Regression score: \"+ str(accuracy_score(y_test, y_pred_log)))\n",
    "    print (\"Logistic Regression confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred_log))\n",
    "    #y_predicted_proba = logReg.predict_proba(x_test_matrix)\n",
    "    #print log_loss(y_test, y_predicted_proba)\n",
    "    print classification_report(y_test, y_pred_log)\n",
    "\n",
    "def linear_svm(x_train_matrix, x_test_matrix, y_train, y_test):\n",
    "    svm1 = LinearSVC(C=1)\n",
    "    svm1.fit(x_train_matrix, x_test_matrix)\n",
    "    y_pred_svc = svm1.predict(y_train)\n",
    "    print (\"SVC score: \" + str(accuracy_score(y_test, y_pred_svc)))\n",
    "    print (\"SVC confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred_svc))\n",
    "    print classification_report(y_test, y_pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_time = time.time()\n",
    "print(\"--- start -> preprocessing:%s seconds ---\" % (preprocessing_time - start_time))\n",
    "\n",
    "\n",
    "print (\"\\n\\n Naive bayes result on word_embedding_sum: \")\n",
    "naive_bayes(X_sum, X_label, Y_sum, Y_label)\n",
    "print (\"\\n\\n Naive bayes result on word_embedding_avg: \")\n",
    "naive_bayes(X_avg, X_label, Y_avg, Y_label)\n",
    "naive_bayes_time = time.time() \n",
    "print(\"--- preprocessing -> naive bayes :%s seconds ---\" % (naive_bayes_time - preprocessing_time))\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "print (\"\\n\\n Logistic Regression result on word_embedding_sum: \")\n",
    "logistic_regression(X_sum, X_label, Y_sum, Y_label)\n",
    "print (\"\\n\\n Logistic Regression result on word_embedding_avg: \")\n",
    "logistic_regression(X_avg, X_label, Y_avg, Y_label)\n",
    "logistic_regression_time = time.time()\n",
    "print(\"--- naive bayes -> logistic regression :%s seconds ---\" % (logistic_regression_time - naive_bayes_time))\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "print (\"\\n\\n Linear SVM result on word_embedding_sum: \")\n",
    "linear_svm(X_sum, X_label, Y_sum, Y_label)\n",
    "print (\"\\n\\n Linear SVM result on word_embedding_avg: \")\n",
    "linear_svm(X_avg, X_label, Y_avg, Y_label)\n",
    "linear_svm_time = time.time()\n",
    "print(\"--- logistic regression -> linear svm :%s seconds ---\" % (linear_svm_time - logistic_regression_time))\n",
    "print(\"--- TOTAL TIME: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df_train['is_duplicate'].tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_embedding1_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
