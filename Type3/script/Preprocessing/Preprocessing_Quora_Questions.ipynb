{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs, csv, sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "question1 = []\n",
    "question2 = []\n",
    "qid1 = []\n",
    "qid2 = []\n",
    "id_ = []\n",
    "is_duplicate = []\n",
    "f = open('/Volumes/Barly/NLP/Project/5k_testing/Type3/data/raw/test.csv','r')\n",
    "reader = csv.DictReader(f)\n",
    "for row in reader:\n",
    "    question1.append(row['question1'].lower())\n",
    "    question2.append(row['question2'].lower())\n",
    "    qid1.append( row['qid1'])\n",
    "    qid2.append( row['qid2'])\n",
    "    id_.append( row['index'])\n",
    "    is_duplicate.append( row['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation_list = list(string.punctuation)\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This','at','have', 'been', 'on', 'too', 'in', 'it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_splitted_to_words(sentence, remove_selective_stopwords = True, lemma=True):\n",
    "    \n",
    "    \n",
    "    sentence = re.sub(r\"what's\", \"\", sentence)\n",
    "    sentence = re.sub(r\"What's\", \"\", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\'ve \", \" have \", sentence)\n",
    "    sentence = re.sub(r\"can't\", \"cannot \", sentence)\n",
    "    sentence = re.sub(r\"n't\", \" not \", sentence)\n",
    "    sentence = re.sub(r\"I'm\", \"I am\", sentence)\n",
    "    sentence = re.sub(r\" m \", \" am \", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are \", sentence)\n",
    "    sentence = re.sub(r\"\\'d \", \" would \", sentence)\n",
    "    sentence = re.sub(r\"\\'ll \", \" will \", sentence)\n",
    "    sentence = re.sub(r\"60k\", \" 60000 \", sentence)\n",
    "    sentence = re.sub(r\" e g \", \" example \", sentence)\n",
    "    sentence = re.sub(r\" b g \", \" bg \", sentence)\n",
    "    sentence = re.sub(r\"\\0s\", \"0\", sentence)\n",
    "    sentence = re.sub(r\" 9 11 \", \"911\", sentence)\n",
    "    sentence = re.sub(r\"e-mail\", \"email\", sentence)\n",
    "    sentence = re.sub(r\"\\s{2,}\", \" \", sentence)\n",
    "    sentence = re.sub(r\"quikly\", \"quickly\", sentence)\n",
    "    sentence = re.sub(r\" usa \", \" America \", sentence)\n",
    "    sentence = re.sub(r\" USA \", \" America \", sentence)\n",
    "    sentence = re.sub(r\" u s \", \" America \", sentence)\n",
    "    sentence = re.sub(r\" uk \", \" England \", sentence)\n",
    "    sentence = re.sub(r\" UK \", \" England \", sentence)\n",
    "    sentence = re.sub(r\"india\", \"India\", sentence)\n",
    "    sentence = re.sub(r\"switzerland\", \"Switzerland\", sentence)\n",
    "    sentence = re.sub(r\"china\", \"China\", sentence)\n",
    "    sentence = re.sub(r\"chinese\", \"Chinese\", sentence) \n",
    "    sentence = re.sub(r\"imrovement\", \"improvement\", sentence)\n",
    "    sentence = re.sub(r\"intially\", \"initially\", sentence)\n",
    "    sentence = re.sub(r\"quora\", \"Quora\", sentence)\n",
    "    sentence = re.sub(r\" dms \", \"direct messages \", sentence)  \n",
    "    sentence = re.sub(r\"demonitization\", \"demonetization\", sentence) \n",
    "    sentence = re.sub(r\"actived\", \"active\", sentence)\n",
    "    sentence = re.sub(r\"kms\", \" kilometers \", sentence)\n",
    "    sentence = re.sub(r\"KMs\", \" kilometers \", sentence)\n",
    "    sentence = re.sub(r\" cs \", \" computer science \", sentence) \n",
    "    sentence = re.sub(r\" upvotes \", \" up votes \", sentence)\n",
    "    sentence = re.sub(r\" iPhone \", \" phone \", sentence)\n",
    "    sentence = re.sub(r\"\\0rs \", \" rs \", sentence) \n",
    "    sentence = re.sub(r\"calender\", \"calendar\", sentence)\n",
    "    sentence = re.sub(r\"ios\", \"operating system\", sentence)\n",
    "    sentence = re.sub(r\"gps\", \"GPS\", sentence)\n",
    "    sentence = re.sub(r\"gst\", \"GST\", sentence)\n",
    "    sentence = re.sub(r\"programing\", \"programming\", sentence)\n",
    "    sentence = re.sub(r\"bestfriend\", \"best friend\", sentence)\n",
    "    sentence = re.sub(r\"dna\", \"DNA\", sentence)\n",
    "    sentence = re.sub(r\"III\", \"3\", sentence) \n",
    "    sentence = re.sub(r\"the US\", \"America\", sentence)\n",
    "    sentence = re.sub(r\"Astrology\", \"astrology\", sentence)\n",
    "    sentence = re.sub(r\"Method\", \"method\", sentence)\n",
    "    sentence = re.sub(r\"Find\", \"find\", sentence) \n",
    "    sentence = re.sub(r\"banglore\", \"Bangalore\", sentence)\n",
    "    sentence = re.sub(r\" J K \", \" JK \", sentence)\n",
    "    sentence = re.sub(r\"[^A-Za-z0-9]\", \" \", sentence)\n",
    "    \n",
    "    sentence = ''.join(ch for ch in sentence if ch not in punctuation_list)\n",
    "    #print \"Step 1: \", sentence\n",
    "    if remove_selective_stopwords:\n",
    "        sentence = sentence.split(\" \")\n",
    "        sentence = [word for word in sentence if word not in  stop_words]\n",
    "        sentence = \" \".join(sentence)\n",
    "        #print \"Step 2: removed stopwords \", sentence\n",
    "        \n",
    "    if lemma:\n",
    "        sentence = sentence.split(\" \")\n",
    "        sentence_lemma = [wordnet_lemmatizer.lemmatize(word) for word in sentence]\n",
    "        sentence = \" \".join(sentence_lemma)\n",
    "        #print \"Step 3: lemmatized\", sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start-time:  1513331841.74 \n",
      "extracted_data_time:  1513331850.26 \n",
      "\n",
      "Total time:  8.52851295471\n"
     ]
    }
   ],
   "source": [
    "df_doc2vec = pd.DataFrame()\n",
    "for c_id, id1 , id2 , q1 , q2, label in zip(id_, qid1, qid2, question1, question2, is_duplicate):\n",
    "    questions = [q1, q2]\n",
    "    try:\n",
    "        cleaned_question = []\n",
    "        for question in questions:\n",
    "            cleaned_sentence = sentences_splitted_to_words(question, True, True)\n",
    "            #print \"cleaned_sentence: \", cleaned_sentence\n",
    "            cleaned_question.append(cleaned_sentence)\n",
    "            \n",
    "        df_doc2vec_temp = pd.DataFrame({'index':[c_id],'qid1': [id1], 'qid2': [id2], 'question1': [q1], 'question2': [q2],\\\n",
    "                                        'cleaned_question1': [cleaned_question[0]], 'cleaned_question2': [cleaned_question[1]],\\\n",
    "                                        'is_duplicate': [label]})\n",
    "        df_doc2vec = pd.concat([df_doc2vec, df_doc2vec_temp])\n",
    "    except:\n",
    "        print c_id\n",
    "extracted_data_time = time.time()\n",
    "\n",
    "print \"start-time: \", start_time, \"\\nextracted_data_time: \", extracted_data_time , \"\\n\\nTotal time: \",(extracted_data_time -start_time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_doc2vec.to_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Preprocessed/preprocessed_test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
