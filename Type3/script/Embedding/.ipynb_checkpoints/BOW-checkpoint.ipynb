{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the processed Training data\n",
    "sentence_1_train_csv = []\n",
    "sentence_2_train_csv = [] \n",
    "clean_sentence_1_train_csv = []\n",
    "clean_sentence_2_train_csv = []\n",
    "is_duplicate_train_csv = []\n",
    "id_train_csv = []\n",
    "qid1_train_csv = []\n",
    "qid2_train_csv = []\n",
    "f = open('/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Preprocessed/preprocessed_train_data.csv','r')\n",
    "reader = csv.DictReader(f)\n",
    "for row in reader:\n",
    "    sentence_1_train_csv.append(row['question1'])\n",
    "    sentence_2_train_csv.append(row['question2'])\n",
    "    clean_sentence_1_train_csv.append(row['cleaned_question1'])\n",
    "    clean_sentence_2_train_csv.append(row['cleaned_question2'])\n",
    "    is_duplicate_train_csv.append( row['is_duplicate'])\n",
    "    id_train_csv.append(row['index'])\n",
    "    qid1_train_csv.append(row['qid1'])\n",
    "    qid2_train_csv.append(row['qid2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the processed Testing data\n",
    "sentence_1_test_csv = []\n",
    "sentence_2_test_csv = [] \n",
    "clean_sentence_1_test_csv = []\n",
    "clean_sentence_2_test_csv = []\n",
    "is_duplicate_test_csv = []\n",
    "id_test_csv = []\n",
    "qid1_test_csv = []\n",
    "qid2_test_csv = []\n",
    "f = open('/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Preprocessed/preprocessed_test_data.csv','r')\n",
    "reader = csv.DictReader(f)\n",
    "for row in reader:\n",
    "    sentence_1_test_csv.append(row['question1'])\n",
    "    sentence_2_test_csv.append(row['question2'])\n",
    "    clean_sentence_1_test_csv.append(row['cleaned_question1'])\n",
    "    clean_sentence_2_test_csv.append(row['cleaned_question2'])\n",
    "    is_duplicate_test_csv.append( row['is_duplicate'])\n",
    "    id_test_csv.append(row['index'])\n",
    "    qid1_test_csv.append(row['qid1'])\n",
    "    qid2_test_csv.append(row['qid2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_q1 = clean_sentence_1_train_csv\n",
    "train_q2 = clean_sentence_2_train_csv\n",
    "test_q1 = clean_sentence_1_test_csv\n",
    "test_q2 = clean_sentence_2_test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to generate n-grams using tfidf vectorizer\n",
    "def vectorizer_sentence(train_q1, train_q2, test_q1 ,test_q2, uni= True, bi =True , uni_bi = True):\n",
    "    # vectorizer = CountVectorizer(ngram_range=(1,2), token_pattern=r'\\b\\w+\\b', stop_words=\"english\", tokenizer=LemmaTokenizer())\n",
    "    x_train = train_q1 + train_q2\n",
    "    if uni:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,1), token_pattern=r'\\b\\w+\\b')\n",
    "    elif bi:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(2,2), token_pattern=r'\\b\\w+\\b')\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,2), token_pattern=r'\\b\\w+\\b')\n",
    "    vectorizer.fit(x_train)                                         # X_train == our data dictionary\n",
    "    train_q1_matrix = vectorizer.transform(train_q1).toarray()                  #transformoing testing data with the our defined dictionary\n",
    "    train_q2_matrix = vectorizer.transform(train_q2).toarray()\n",
    "    \n",
    "    df_train_cos['pred_'+str(uni)+str(bi)+str(uni_bi)] = map(cos_, train_q1_matrix, train_q2_matrix)\n",
    "    \n",
    "    \n",
    "    test_q1_matrix = vectorizer.transform(test_q1).toarray()\n",
    "    test_q2_matrix = vectorizer.transform(test_q2).toarray()\n",
    "    \n",
    "    df_test_cos['pred_'+str(uni)+str(bi)+str(uni_bi)] = map(cos_, test_q1_matrix, test_q2_matrix)\n",
    "\n",
    "    \n",
    "    train_matrix = np.concatenate((train_q1_matrix,train_q2_matrix), axis=1) #Concatenating the vector representation of train question1 and question2\n",
    "    test_matrix = np.concatenate((test_q1_matrix,test_q2_matrix), axis=1)\n",
    "    return train_matrix,test_matrix, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_cos = pd.DataFrame()\n",
    "df_train_cos['cleaned_question1'] = clean_sentence_1_train_csv\n",
    "df_train_cos['cleaned_question2'] = clean_sentence_2_train_csv\n",
    "df_train_cos['is_duplicate'] = is_duplicate_train_csv\n",
    "df_train_cos['is_duplicate'] = df_train_cos['is_duplicate'].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "df_test_cos = pd.DataFrame()\n",
    "df_test_cos['cleaned_question1'] = clean_sentence_1_test_csv\n",
    "df_test_cos['cleaned_question2'] = clean_sentence_2_test_csv\n",
    "df_test_cos['is_duplicate'] = is_duplicate_test_csv\n",
    "df_test_cos['is_duplicate'] = df_test_cos['is_duplicate'].apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating the cisine similarity between two vectors\n",
    "def cos_(elem1, elem2):\n",
    "    elem1 = elem1.reshape(1,-1)\n",
    "    elem2 = elem2.reshape(1,-1)\n",
    "    a = cosine_similarity(elem1, elem2)[0][0]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the n-gram features for concatenated train questions and test questions\n",
    "X_train_matrix_uni, X_test_matrix_uni, uni_features = vectorizer_sentence(train_q1, train_q2, test_q1 ,test_q2, True, False, False)\n",
    "X_train_matrix_bi, X_test_matrix_bi, bi_features  = vectorizer_sentence(train_q1, train_q2, test_q1 ,test_q2, False, True, False)\n",
    "X_train_matrix_unibi, X_test_matrix_unibi, unibi_features  = vectorizer_sentence(train_q1, train_q2, test_q1 ,test_q2, False, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uni_features), len(bi_features), len(unibi_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving the n-grams into csv\n",
    "df_train_Uni = pd.DataFrame(X_train_matrix_uni, columns=uni_features*2)\n",
    "df_train_Uni['is_duplicate'] = is_duplicate_train_csv\n",
    "df_train_Uni.to_csv('/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Unigram/train.csv', index=False)\n",
    "\n",
    "df_test_Uni = pd.DataFrame(X_test_matrix_uni, columns=uni_features*2)\n",
    "df_test_Uni['is_duplicate'] = is_duplicate_test_csv\n",
    "df_test_Uni.to_csv('/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Unigram/test.csv', index=False)\n",
    "\n",
    "df_train_Bi = pd.DataFrame(X_train_matrix_bi, columns=bi_features*2)\n",
    "df_train_Bi['is_duplicate'] = is_duplicate_train_csv\n",
    "df_train_Bi.to_csv('/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Bigram/train.csv', index=False)\n",
    "\n",
    "df_test_Bi = pd.DataFrame(X_test_matrix_bi, columns=bi_features*2)\n",
    "df_test_Bi['is_duplicate'] = is_duplicate_test_csv\n",
    "df_test_Bi.to_csv('/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Bigram/test.csv', index=False)\n",
    "\n",
    "df_train_UniBi = pd.DataFrame(X_train_matrix_unibi, columns=unibi_features*2)\n",
    "df_train_UniBi['is_duplicate'] = is_duplicate_train_csv\n",
    "df_train_UniBi.to_csv('/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Uni_Bigram/train.csv', index=False)\n",
    "\n",
    "df_test_UniBi = pd.DataFrame(X_test_matrix_unibi, columns=unibi_features*2)\n",
    "df_test_UniBi['is_duplicate'] = is_duplicate_test_csv\n",
    "df_test_UniBi.to_csv('/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Uni_Bigram/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = is_duplicate_train_csv\n",
    "Y_test = is_duplicate_test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Performing cross validation on classifiers: Naive Bayes, Logistic Regression, and Linear SVM\n",
    "\n",
    "def naive_bayes(x_train_matrix, x_test_matrix, y_train, y_test):    #NAIVE BAYERS MODEL                                                                \n",
    "    nb = MultinomialNB(alpha=2.8)\n",
    "    nb.fit(x_train_matrix, y_train)                                 #train the model\n",
    "    y_pred = nb.predict(x_test_matrix)                              #make predictions for X_test\n",
    "    print (\"Naive Bayes Model score: \" + str(accuracy_score(y_test, y_pred)))         \n",
    "    print (\"Naive Bayes Model confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred))\n",
    "    y_predicted_proba = nb.predict_proba(x_test_matrix)\n",
    "    print log_loss(y_test, y_predicted_proba)\n",
    "\n",
    "def logistic_regression(x_train_matrix, x_test_matrix, y_train, y_test): \n",
    "    logReg = LogisticRegressionCV(Cs=[0.01, 0.1, 1, 10, 100], cv=5, solver='saga' )\n",
    "    logReg.fit(x_train_matrix, y_train)\n",
    "    y_pred_log = logReg.predict(x_test_matrix)\n",
    "    print (\"Logistic Regression score: \"+ str(accuracy_score(y_test, y_pred_log)))\n",
    "    print (\"Logistic Regression confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred_log))\n",
    "    y_predicted_proba = logReg.predict_proba(x_test_matrix)\n",
    "    print log_loss(y_test, y_predicted_proba)\n",
    "    \n",
    "def linear_svm(x_train_matrix, x_test_matrix, y_train, y_test):\n",
    "    \n",
    "    param_grid = [\\\n",
    "                  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\\\n",
    "                  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}]\n",
    "    svc = svm.SVC()\n",
    "    svm1 = GridSearchCV(estimator=svc, param_grid=param_grid, n_jobs=1, cv = 5)\n",
    " \n",
    "    #svm1 = LinearSVC(C=100)\n",
    "    svm1.fit(x_train_matrix, y_train)\n",
    "    y_pred_svc = svm1.predict(x_test_matrix)\n",
    "    print (\"SVC score: \" + str(accuracy_score(y_test, y_pred_svc)))\n",
    "    print (\"SVC confusionmatrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing_time = time.time()\n",
    "#print(\"--- start -> preprocessing:%s seconds ---\" % (preprocessing_time - start_time))\n",
    "\n",
    "\n",
    "print(\" Naive Bayes: Unigram \\n\")\n",
    "naive_bayes(X_train_matrix_uni, X_test_matrix_uni, Y_train, Y_test)\n",
    "print(\" Naive Bayes: Bigram \\n\")\n",
    "naive_bayes(X_train_matrix_bi, X_test_matrix_bi, Y_train, Y_test)\n",
    "print(\" Naive Bayes: UniBigram \\n\")\n",
    "naive_bayes(X_train_matrix_unibi, X_test_matrix_unibi, Y_train, Y_test)\n",
    "naive_bayes_time = time.time() \n",
    "print(\"--- preprocessing -> naive bayes :%s seconds ---\" % (naive_bayes_time - preprocessing_time))\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "print(\"Logistic Regresssion: Unigram\")\n",
    "logistic_regression(X_train_matrix_uni, X_test_matrix_uni, Y_train, Y_test)\n",
    "print(\"Logistic Regresssion: Bigram\")\n",
    "logistic_regression(X_train_matrix_bi, X_test_matrix_bi, Y_train, Y_test)\n",
    "print(\"Logistic Regresssion: UniBigram\")\n",
    "logistic_regression(X_train_matrix_unibi, X_test_matrix_unibi, Y_train, Y_test)\n",
    "logistic_regression_time = time.time()\n",
    "print(\"--- naive bayes -> logistic regression :%s seconds ---\" % (logistic_regression_time - naive_bayes_time))\n",
    "\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "print(\"Linear SVM: Unigram\")\n",
    "linear_svm(X_train_matrix_uni, X_test_matrix_uni, Y_train, Y_test)\n",
    "\n",
    "print(\"Linear SVM: Bigram\")\n",
    "linear_svm(X_train_matrix_bi, X_test_matrix_bi, Y_train, Y_test)\n",
    "print(\"Linear SVM: UniBigram\")\n",
    "linear_svm(X_train_matrix_unibi, X_test_matrix_unibi, Y_train, Y_test)\n",
    "linear_svm_time = time.time()\n",
    "print(\"--- logistic regression -> linear svm :%s seconds ---\" % (linear_svm_time - logistic_regression_time))\n",
    "#print(\"--- TOTAL TIME: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculating the best thresholds for n-grams for Cosine similarity scores \n",
    "\n",
    "def conv(elem):\n",
    "    a = re.compile(r\"[-+]?\\d*\\.\\d+|\\d+\")\n",
    "    return re.findall(a, elem)[0]\n",
    "\n",
    "threshold_list = [0.6, 0.5, 0.4, 0.3]\n",
    "target_names = [0,1]\n",
    "type_ = [\"TrueFalseFalse\", \"FalseTrueFalse\", \"FalseFalseTrue\"]\n",
    "for type_c in type_:\n",
    "    for threshold in threshold_list:\n",
    "\n",
    "        print \"Threshold: \", threshold\n",
    "        df_cos_test = pd.DataFrame()\n",
    "        df_cos_test['thre_'+str(threshold)] = np.where(df_test_cos['pred_'+type_c]>=threshold, 1, 0)\n",
    "\n",
    "        print confusion_matrix(df_test_cos['is_duplicate'], df_cos_test['thre_'+str(threshold)]\\\n",
    "                                          , labels=target_names).ravel()\n",
    "\n",
    "        print classification_report(df_test_cos['is_duplicate'], df_cos_test['thre_'+str(threshold)])\n",
    "\n",
    "        print \"Accuracy: \" ,accuracy_score(df_test_cos['is_duplicate'], df_cos_test['thre_'+str(threshold)])\n",
    "        print \"\\n*******************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
