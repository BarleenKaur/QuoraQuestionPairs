{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import types\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from itertools import chain\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import codecs, csv, sys\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "punctuation_list = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Preprocessed/preprocessed_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "sentence_1 = df_train['question1']\n",
    "sentence_2 = df_train['question2']\n",
    "clean_sentence_1 = df_train['cleaned_question1']\n",
    "clean_sentence_2 = df_train['cleaned_question2']\n",
    "is_duplicate = df_train['is_duplicate']\n",
    "id_ = df_train['index']\n",
    "qid1 = df_train['qid1']\n",
    "qid2 = df_train['qid2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"/Volumes/Barly/NLP/Project/GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_1_sum = []\n",
    "embedding_2_sum = []\n",
    "embedding_1_avg = []\n",
    "embedding_2_avg = []\n",
    "cos_sum = []\n",
    "cos_avg = []\n",
    "for c_id, q1 , q2, label in zip(id_, clean_sentence_1, clean_sentence_2, is_duplicate):\n",
    "    questions = [q1, q2]\n",
    "    embeddings_sum = []\n",
    "    embeddings_avg = []\n",
    "    for index, question in enumerate(questions): \n",
    "        try:\n",
    "            if isinstance(question, types.StringType) and not (question and question.strip()):  #checking for Nan values\n",
    "                print \"question: \", question, \"cid: \", c_id \n",
    "                word2vec_embed = np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "                #print word2vec_embed\n",
    "                if index == 0:\n",
    "                    embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                    embedding_1_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                else:\n",
    "                    embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                    embedding_2_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "\n",
    "                word2vec_embed_avg = word2vec_embed\n",
    "\n",
    "            elif question != question:\n",
    "                print \"question: \", question, \"cid: \", c_id \n",
    "                word2vec_embed = np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "                #print word2vec_embed\n",
    "                if index == 0:\n",
    "                    embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                    embedding_1_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                else:\n",
    "                    embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                    embedding_2_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "\n",
    "                word2vec_embed_avg = word2vec_embed\n",
    "\n",
    "\n",
    "            else:\n",
    "                #print question\n",
    "                if question not in punctuation_list:\n",
    "                    question_token = tokenizer.tokenize(question)\n",
    "                    word2vec_embed = np.zeros(300)\n",
    "                    no_of_words = 0.0\n",
    "                    for word in question_token:\n",
    "                        if word in model.vocab:\n",
    "                            word2vec_embed += model.word_vec(word) \n",
    "                        else:\n",
    "                            word2vec_embed += np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "                        no_of_words += 1\n",
    "\n",
    "                    word2vec_embed_avg = word2vec_embed/no_of_words\n",
    "\n",
    "                    if index == 0:\n",
    "                        embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                        embedding_1_avg.append(word2vec_embed_avg.reshape(1,-1)[0])\n",
    "                    else:\n",
    "                        embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                        embedding_2_avg.append(word2vec_embed_avg.reshape(1,-1)[0])\n",
    "        except:\n",
    "            print \"question: \", question, \"cid: \", c_id \n",
    "            word2vec_embed = np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "            #print word2vec_embed\n",
    "            if index == 0:\n",
    "                embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                embedding_1_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "            else:\n",
    "                embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                embedding_2_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "\n",
    "            word2vec_embed_avg = word2vec_embed\n",
    "            \n",
    "        embeddings_sum.append(word2vec_embed.reshape(1,-1))\n",
    "        embeddings_avg.append(word2vec_embed_avg.reshape(1,-1))  \n",
    "    cos_sum.append(cosine_similarity(embeddings_sum[0],embeddings_sum[1])[0])\n",
    "    #print \"cos_avg: \", cosine_similarity(embeddings_avg[0],embeddings_avg[1])[0]\n",
    "    cos_avg.append(cosine_similarity(embeddings_avg[0],embeddings_avg[1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word2vec = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word2vec['id'] = id_\n",
    "df_word2vec['qid1'] = qid1\n",
    "df_word2vec['qid2'] = qid2\n",
    "df_word2vec['embedding_1_sum'] = embedding_1_sum\n",
    "df_word2vec['embedding_2_sum'] = embedding_2_sum\n",
    "df_word2vec['embedding_1_avg'] = embedding_1_avg\n",
    "df_word2vec['embedding_2_avg'] = embedding_2_avg\n",
    "df_word2vec['cleaned_question1'] = clean_sentence_1\n",
    "df_word2vec['cleaned_question2'] = clean_sentence_2\n",
    "df_word2vec['is_duplicate'] = is_duplicate\n",
    "df_word2vec['cos_sum'] = cos_sum\n",
    "df_word2vec['cos_avg'] = cos_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word2vec.to_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type3/data/Word2Vec/test_word2vec.csv\", index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word2vec.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold_list = [0.5 , 0.6 , 0.7, 0.8, 0.9]\n",
    "cosine_sum = df_word2vec['cos_sum']\n",
    "cosine_avg = df_word2vec['cos_avg']\n",
    "y_true = is_duplicate\n",
    "for threshold in threshold_list:\n",
    "    is_dup_pred_sum = []\n",
    "    is_dup_pred_avg = []\n",
    "    for cos_sum , cos_avg in zip(cosine_sum,cosine_avg):\n",
    "        if cos_sum > threshold and cos_avg > threshold:\n",
    "            is_dup_pred_sum.append(1)\n",
    "            is_dup_pred_avg.append(1)\n",
    "        elif cos_sum > threshold and cos_avg < threshold:\n",
    "            is_dup_pred_sum.append(1)\n",
    "            is_dup_pred_avg.append(0)\n",
    "        elif cos_sum < threshold and cos_avg > threshold:\n",
    "            is_dup_pred_sum.append(0)\n",
    "            is_dup_pred_avg.append(1)\n",
    "        else:\n",
    "            is_dup_pred_sum.append(0)\n",
    "            is_dup_pred_avg.append(0)\n",
    "            \n",
    "    df_word2vec['is_dup_pred_sum'+ str(threshold)] = is_dup_pred_sum\n",
    "    df_word2vec['is_dup_pred_avg'+ str(threshold)] = is_dup_pred_avg\n",
    "    y_predict = [is_dup_pred_sum, is_dup_pred_avg]\n",
    "    target_names = [0, 1]\n",
    "    for y_pred in y_predict:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=target_names).ravel()\n",
    "        print \"Threshold: \", \n",
    "        print confusion_matrix(y_true, y_pred, labels=target_names)\n",
    "        accuracy = float(tn + tp)/(tn + fp + fn + tp)\n",
    "        print \"Accuracy: \", accuracy\n",
    "        print classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word2vec.loc[df_word2vec['is_duplicate'] == 1, 'cos_sum'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word2vec.loc[df_word2vec['is_duplicate'] == 0, 'cos_sum'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sentence_1_csv = []\n",
    "sentence_2_csv = [] \n",
    "clean_sentence_1_csv = []\n",
    "clean_sentence_2_csv = []\n",
    "sentence_1_english_stopwords_csv = []\n",
    "sentence_2_english_stopwords_csv = []\n",
    "sentence_1_without_stopwords_csv = []\n",
    "sentence_2_without_stopwords_csv = []\n",
    "is_duplicate_csv = []\n",
    "id_csv = []\n",
    "qid1_csv = []\n",
    "qid2_csv = []\n",
    "f = open('checking.csv','r')\n",
    "reader = csv.DictReader(f)\n",
    "for row in reader:\n",
    "    sentence_1_csv.append(row['question1'])\n",
    "    sentence_2_csv.append(row['question2'])\n",
    "    clean_sentence_1_csv.append(row['cleaned_question1'])\n",
    "    clean_sentence_2_csv.append(row['cleaned_question2'])\n",
    "    sentence_1_english_stopwords_csv.append(row['cleaned_question1_without_english_stopwords'])\n",
    "    sentence_2_english_stopwords_csv.append(row['cleaned_question2_without_english_stopwords'])\n",
    "    sentence_1_without_stopwords_csv.append(row['cleaned_question1_without_stopwords'])\n",
    "    sentence_2_without_stopwords_csv.append(row['cleaned_question2_without_stopwords'])\n",
    "    is_duplicate_csv.append( row['is_duplicate'])\n",
    "    id_csv.append(row['index'])\n",
    "    qid1_csv.append(row['qid1'])\n",
    "    qid2_csv.append(row['qid2'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(\"/Volumes/Barly/NLP/Project/data/train_word2vec_selective_stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new.columns.values[8]='cleaned_question1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new.columns.values[9]='cleaned_question2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_again = pd.read_csv(\"/Volumes/Barly/NLP/Project/data/train_word2vec_selective_stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_again.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'someone', 0.6657355427742004),\n",
       " (u'persons', 0.555971086025238),\n",
       " (u'woman', 0.5470173358917236),\n",
       " (u'somebody', 0.5459041595458984),\n",
       " (u'peson', 0.5421414375305176),\n",
       " (u'man', 0.5342026948928833),\n",
       " (u'people', 0.5083408951759338),\n",
       " (u'anyone', 0.5061744451522827),\n",
       " (u'guy', 0.48752474784851074),\n",
       " (u'Someone', 0.473005473613739)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word(\"person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_1_sum = []\n",
    "embedding_2_sum = []\n",
    "embedding_1_avg = []\n",
    "embedding_2_avg = []\n",
    "cos_sum = []\n",
    "cos_avg = []\n",
    "\n",
    "q1 = \"good bad ugly\"\n",
    "q2 = \"good bad ugly\"\n",
    "questions = [q1, q2]\n",
    "embeddings_sum = []\n",
    "embeddings_avg = []\n",
    "for index, question in enumerate(questions): \n",
    "    try:\n",
    "        if isinstance(question, types.StringType) and not (question and question.strip()):  #checking for Nan values\n",
    "            #print \"question: \", question, \"cid: \", c_id \n",
    "            word2vec_embed = np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "            #print word2vec_embed\n",
    "            if index == 0:\n",
    "                embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                embedding_1_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "            else:\n",
    "                embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                embedding_2_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "\n",
    "            word2vec_embed_avg = word2vec_embed\n",
    "\n",
    "        elif question != question:\n",
    "            #print \"question: \", question, \"cid: \", c_id \n",
    "            word2vec_embed = np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "            #print word2vec_embed\n",
    "            if index == 0:\n",
    "                embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                embedding_1_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "            else:\n",
    "                embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                embedding_2_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "\n",
    "            word2vec_embed_avg = word2vec_embed\n",
    "\n",
    "\n",
    "        else:\n",
    "            #print question\n",
    "            if question not in punctuation_list:\n",
    "                question_token = tokenizer.tokenize(question)\n",
    "                word2vec_embed = np.zeros(300)\n",
    "                no_of_words = 0.0\n",
    "                for word in question_token:\n",
    "                    if word in model.vocab:\n",
    "                        word2vec_embed += model.word_vec(word) \n",
    "                    else:\n",
    "                        word2vec_embed += np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "                    no_of_words += 1\n",
    "\n",
    "                word2vec_embed_avg = word2vec_embed/no_of_words\n",
    "\n",
    "                if index == 0:\n",
    "                    embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                    embedding_1_avg.append(word2vec_embed_avg.reshape(1,-1)[0])\n",
    "                else:\n",
    "                    embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                    embedding_2_avg.append(word2vec_embed_avg.reshape(1,-1)[0])\n",
    "    except:\n",
    "       # print \"question: \", question, \"cid: \", c_id \n",
    "        word2vec_embed = np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "        #print word2vec_embed\n",
    "        if index == 0:\n",
    "            embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "            embedding_1_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "        else:\n",
    "            embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "            embedding_2_avg.append(word2vec_embed.reshape(1,-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_1_sum[0] == embedding_2_sum[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.34231286e+00,  -1.84147779e+00,   1.56909702e+00,\n",
       "         6.58666932e-03,  -2.65070898e+00,   3.34537603e-01,\n",
       "         1.94169295e+00,  -6.42120417e-01,   3.38215396e-01,\n",
       "         9.51196017e-01,   5.59820187e-01,   1.87229526e+00,\n",
       "         8.50026367e-01,   5.39513498e-01,  -1.79232180e+00,\n",
       "        -3.50459828e-01,  -3.37463173e-01,   1.17354772e+00,\n",
       "        -1.53024970e+00,   1.48999591e+00,  -1.92368236e+00,\n",
       "         1.31118948e+00,   1.55461251e+00,  -5.95992248e-01,\n",
       "        -7.15690944e-01,  -8.32676577e-01,   9.65442679e-01,\n",
       "        -2.94460119e-01,   1.67142790e+00,  -8.87306024e-01,\n",
       "        -8.05730862e-02,   8.48046365e-01,  -1.08363335e+00,\n",
       "        -1.52701334e+00,  -5.46664606e-01,   2.21281277e-01,\n",
       "         2.66236301e+00,   1.10463396e+00,  -2.37112511e-01,\n",
       "         3.17704280e-01,  -1.52239496e-01,  -1.19939742e+00,\n",
       "         2.42939231e-01,  -1.64809677e+00,  -1.29838517e+00,\n",
       "         1.30219205e+00,   7.09328621e-02,   1.41608623e-01,\n",
       "        -1.84364747e+00,  -5.45871356e-02,  -5.25263354e-01,\n",
       "        -2.35122325e-01,   1.73483693e-01,   9.17673824e-01,\n",
       "         6.78575092e-01,  -5.40998924e-01,  -1.77980228e+00,\n",
       "        -2.31377651e+00,  -7.76498862e-03,   2.56331071e+00,\n",
       "        -1.24822289e+00,  -8.73237540e-01,  -1.37583288e+00,\n",
       "        -1.28210268e+00,   1.38741233e+00,   1.14682390e-01,\n",
       "        -1.82235371e-01,   9.10819540e-01,  -2.40545262e+00,\n",
       "         6.35384391e-01,   2.52453332e+00,   1.26087140e+00,\n",
       "        -1.45957487e+00,  -1.06711534e-01,   8.55421977e-01,\n",
       "        -3.38585744e-01,   5.73393906e-01,  -1.21520623e+00,\n",
       "        -6.33351051e-01,   1.13030758e+00,   9.73475193e-01,\n",
       "         2.13514949e+00,  -9.60738175e-01,   5.43028756e-01,\n",
       "        -5.26406726e-01,  -3.54936825e-01,  -6.52480160e-01,\n",
       "         1.44581003e+00,  -5.59271476e-01,   2.61956490e-01,\n",
       "        -2.59221656e+00,   1.83722210e+00,   1.70909481e-01,\n",
       "        -2.93628774e-01,   1.78123542e+00,   8.20975649e-01,\n",
       "        -4.95502532e-01,   2.36592128e-01,   1.48762761e+00,\n",
       "         4.35521445e-01,   3.53308010e-01,  -5.56351408e-01,\n",
       "         7.80183308e-01,  -2.73525845e-01,  -1.23329676e+00,\n",
       "        -1.43678475e+00,   7.50107761e-03,   4.56136826e-01,\n",
       "        -9.21054660e-01,   9.48750163e-01,  -5.47532917e-01,\n",
       "        -2.09638372e+00,   5.97586510e-01,   1.70231198e-01,\n",
       "         1.34577886e+00,  -3.96034598e-01,   7.29099873e-01,\n",
       "        -2.83604271e-01,   1.77035396e-01,   2.41298680e-01,\n",
       "        -4.27134959e-01,  -1.37435577e+00,  -3.04452722e+00,\n",
       "        -7.55573128e-01,   3.08560703e-01,   1.90813488e-01,\n",
       "         5.26803964e-01,   2.65502685e+00,  -4.26783129e-02,\n",
       "         8.99953943e-01,  -3.47263643e-01,  -6.67088318e-01,\n",
       "        -6.89836391e-01,   4.60903337e-01,   8.97999450e-02,\n",
       "        -3.79165660e-01,   9.62714438e-01,  -1.04949334e+00,\n",
       "         9.02703280e-01,   4.35847669e-02,   9.50466551e-01,\n",
       "        -1.33516103e+00,   1.57396153e+00,   1.83473502e-01,\n",
       "        -2.30999482e+00,  -3.11611588e-02,  -1.24239871e+00,\n",
       "        -3.75560109e-01,  -1.55972351e-01,  -7.31826401e-01,\n",
       "        -2.70207310e-01,  -9.52912741e-01,  -9.21197419e-01,\n",
       "         4.31501372e-01,  -9.92985992e-01,   1.07320932e+00,\n",
       "        -1.16714388e+00,  -1.44548215e+00,  -1.68988337e-01,\n",
       "        -2.20442780e+00,   9.87575574e-01,   2.61318288e+00,\n",
       "         1.09592467e-01,  -8.16042110e-02,  -4.11957054e-01,\n",
       "        -2.42817619e+00,   6.75696661e-01,  -1.05631548e-01,\n",
       "         1.57083313e+00,   2.15539493e-01,  -1.34025035e+00,\n",
       "        -1.36244120e+00,  -1.22448839e+00,  -1.23459233e+00,\n",
       "         2.15906643e+00,   1.54352783e+00,   2.82300353e+00,\n",
       "         9.32090957e-01,  -9.72052447e-02,  -1.29064186e-01,\n",
       "        -4.99060927e-01,  -3.51729776e-01,   4.95565381e-02,\n",
       "         2.24812132e-01,  -2.63562846e-01,   9.95291718e-01,\n",
       "         8.50138349e-03,   1.17912852e+00,   1.59129562e+00,\n",
       "         1.33320925e+00,  -2.10460969e-01,  -9.27793711e-02,\n",
       "         9.37481990e-01,  -1.32124900e+00,   1.71986638e+00,\n",
       "        -2.46938112e-01,  -2.43896824e+00,  -1.02842523e+00,\n",
       "        -1.73447565e+00,  -7.79015047e-01,  -1.82792181e+00,\n",
       "        -1.08284229e+00,  -1.25451703e+00,  -7.81085409e-01,\n",
       "         1.11741720e+00,  -7.09364784e-01,  -1.39966552e+00,\n",
       "         1.06641855e+00,  -2.42425055e-01,  -5.42495239e-01,\n",
       "        -7.48647045e-01,   1.20341396e+00,  -3.01438949e-01,\n",
       "         3.85127888e-02,  -9.12296186e-01,  -7.15420727e-01,\n",
       "         1.13316074e+00,  -8.09191548e-01,  -5.59734030e-01,\n",
       "         9.02774181e-01,   1.66667389e-02,  -8.64790399e-02,\n",
       "         2.00745362e-01,  -2.86434788e-01,  -1.58612766e-01,\n",
       "        -2.00288986e-02,  -2.66613092e-01,  -9.31747193e-01,\n",
       "        -5.48933320e-01,  -1.00660947e-01,  -4.58437210e-01,\n",
       "        -8.05723097e-02,   1.16011855e+00,  -1.01218804e+00,\n",
       "        -7.82800065e-01,  -1.74772727e+00,  -2.25047861e+00,\n",
       "         8.45950015e-01,  -4.80648984e-01,   1.22350391e+00,\n",
       "         3.42071581e-01,  -4.00524120e-01,   7.16489446e-02,\n",
       "        -7.09663635e-01,   1.59889033e+00,   7.88785290e-01,\n",
       "        -4.06929502e-01,   6.02016694e-01,   1.70997937e+00,\n",
       "        -7.42417195e-01,   1.17566143e+00,  -4.01861628e-01,\n",
       "        -1.30119339e+00,  -5.85516062e-01,   2.06116214e+00,\n",
       "        -1.82262469e+00,  -1.25336388e+00,  -6.37297161e-02,\n",
       "        -2.07714045e-01,  -1.23342252e+00,  -6.06281756e-01,\n",
       "        -4.77929660e-01,  -7.76034951e-03,  -5.85363260e-01,\n",
       "        -5.72319976e-01,   2.26712376e-01,  -7.60630100e-01,\n",
       "        -9.34757369e-01,  -5.18535222e-01,  -1.17419563e-01,\n",
       "        -2.59746625e-01,  -6.01137567e-01,  -5.61564435e-01,\n",
       "         8.91233613e-01,  -6.78995813e-02,   8.59434240e-01,\n",
       "        -2.33961508e-01,   7.44400758e-02,  -1.84580809e+00,\n",
       "         4.36370440e-01,   1.29454116e+00,   1.35544881e+00,\n",
       "        -3.17913288e-04,  -1.84217307e+00,  -1.26115906e+00,\n",
       "        -6.03626683e-01,   6.23426630e-01,  -1.59623938e+00,\n",
       "        -1.77998169e+00,   8.81703307e-02,   8.98152247e-02,\n",
       "         1.07515749e+00,  -4.22315957e-01,   1.69812703e+00,\n",
       "        -1.84568896e+00,  -9.32866020e-01,  -1.65919067e+00,\n",
       "         3.84095307e-03,   5.72217608e-01,  -7.12925188e-01])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_1_sum[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type3/Results/test_word2vec_classifier.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5a787b0db2f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Threshold: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "threshold_list = [0.5 , 0.6 , 0.7, 0.8, 0.9]\n",
    "cosine_sum = df_new['cos_sum']\n",
    "cosine_avg = df_new['cos_avg']\n",
    "y_true = df_new['is_duplicate']\n",
    "for threshold in threshold_list:\n",
    "    is_dup_pred_sum = []\n",
    "    is_dup_pred_avg = []\n",
    "    for cos_sum , cos_avg in zip(cosine_sum,cosine_avg):\n",
    "        if cos_sum > threshold and cos_avg > threshold:\n",
    "            is_dup_pred_sum.append(1)\n",
    "            is_dup_pred_avg.append(1)\n",
    "        elif cos_sum > threshold and cos_avg < threshold:\n",
    "            is_dup_pred_sum.append(1)\n",
    "            is_dup_pred_avg.append(0)\n",
    "        elif cos_sum < threshold and cos_avg > threshold:\n",
    "            is_dup_pred_sum.append(0)\n",
    "            is_dup_pred_avg.append(1)\n",
    "        else:\n",
    "            is_dup_pred_sum.append(0)\n",
    "            is_dup_pred_avg.append(0)\n",
    "            \n",
    "    df_new['is_dup_pred_sum'+ str(threshold)] = is_dup_pred_sum\n",
    "    df_new['is_dup_pred_avg'+ str(threshold)] = is_dup_pred_avg\n",
    "    y_predict = [is_dup_pred_sum, is_dup_pred_avg]\n",
    "    target_names = [0, 1]\n",
    "    for y_pred in y_predict:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=target_names).ravel()\n",
    "        print \"Threshold: \", \n",
    "        print confusion_matrix(y_true, y_pred, labels=target_names)\n",
    "        accuracy = float(tn + tp)/(tn + fp + fn + tp)\n",
    "        print \"Accuracy: \", accuracy\n",
    "        print classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
