{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import types\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from itertools import chain\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import codecs, csv, sys\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "punctuation_list = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the processsed training dataset\n",
    "df_train = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type2/data/Preprocessed/preprocessed_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "sentence_1 = df_train['question1']\n",
    "sentence_2 = df_train['question2']\n",
    "clean_sentence_1 = df_train['cleaned_question1']\n",
    "clean_sentence_2 = df_train['cleaned_question2']\n",
    "is_duplicate = df_train['is_duplicate']\n",
    "id_ = df_train['index']\n",
    "qid1 = df_train['qid1']\n",
    "qid2 = df_train['qid2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the pre-trained word2vec model\n",
    "model = KeyedVectors.load_word2vec_format(\"/Volumes/Barly/NLP/Project/GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating vectors for question1 and question2 using word2vec sum and word2vec averaging approaches\n",
    "# Calculating the cosine similarity scores between two vectors\n",
    "embedding_1_sum = []\n",
    "embedding_2_sum = []\n",
    "embedding_1_avg = []\n",
    "embedding_2_avg = []\n",
    "cos_sum = []\n",
    "cos_avg = []\n",
    "for c_id, q1 , q2, label in zip(id_, clean_sentence_1, clean_sentence_2, is_duplicate):\n",
    "    questions = [q1, q2]\n",
    "    embeddings_sum = []\n",
    "    embeddings_avg = []\n",
    "    for index, question in enumerate(questions): \n",
    "        try:\n",
    "            if isinstance(question, types.StringType) and not (question and question.strip()):  #checking for Nan values\n",
    "                print \"question: \", question, \"cid: \", c_id \n",
    "                word2vec_embed = np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "                #print word2vec_embed\n",
    "                if index == 0:\n",
    "                    embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                    embedding_1_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                else:\n",
    "                    embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                    embedding_2_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "\n",
    "                word2vec_embed_avg = word2vec_embed\n",
    "\n",
    "            elif question != question:\n",
    "                print \"question: \", question, \"cid: \", c_id \n",
    "                word2vec_embed = np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "                #print word2vec_embed\n",
    "                if index == 0:\n",
    "                    embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                    embedding_1_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                else:\n",
    "                    embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                    embedding_2_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "\n",
    "                word2vec_embed_avg = word2vec_embed\n",
    "\n",
    "\n",
    "            else:\n",
    "                #print question\n",
    "                if question not in punctuation_list:\n",
    "                    question_token = tokenizer.tokenize(question)\n",
    "                    word2vec_embed = np.zeros(300)\n",
    "                    no_of_words = 0.0\n",
    "                    for word in question_token:\n",
    "                        if word in model.vocab:\n",
    "                            word2vec_embed += model.word_vec(word) \n",
    "                        else:\n",
    "                            word2vec_embed += np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "                        no_of_words += 1\n",
    "\n",
    "                    word2vec_embed_avg = word2vec_embed/no_of_words\n",
    "\n",
    "                    if index == 0:\n",
    "                        embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                        embedding_1_avg.append(word2vec_embed_avg.reshape(1,-1)[0])\n",
    "                    else:\n",
    "                        embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                        embedding_2_avg.append(word2vec_embed_avg.reshape(1,-1)[0])\n",
    "        except:\n",
    "            print \"question: \", question, \"cid: \", c_id \n",
    "            word2vec_embed = np.random.uniform(low=-1.0, high=1.0, size=(300,))\n",
    "            #print word2vec_embed\n",
    "            if index == 0:\n",
    "                embedding_1_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                embedding_1_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "            else:\n",
    "                embedding_2_sum.append(word2vec_embed.reshape(1,-1)[0])\n",
    "                embedding_2_avg.append(word2vec_embed.reshape(1,-1)[0])\n",
    "\n",
    "            word2vec_embed_avg = word2vec_embed\n",
    "            \n",
    "        embeddings_sum.append(word2vec_embed.reshape(1,-1))\n",
    "        embeddings_avg.append(word2vec_embed_avg.reshape(1,-1))  \n",
    "    cos_sum.append(cosine_similarity(embeddings_sum[0],embeddings_sum[1])[0])\n",
    "    #print \"cos_avg: \", cosine_similarity(embeddings_avg[0],embeddings_avg[1])[0]\n",
    "    cos_avg.append(cosine_similarity(embeddings_avg[0],embeddings_avg[1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word2vec = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word2vec['id'] = id_\n",
    "df_word2vec['qid1'] = qid1\n",
    "df_word2vec['qid2'] = qid2\n",
    "df_word2vec['embedding_1_sum'] = embedding_1_sum\n",
    "df_word2vec['embedding_2_sum'] = embedding_2_sum\n",
    "df_word2vec['embedding_1_avg'] = embedding_1_avg\n",
    "df_word2vec['embedding_2_avg'] = embedding_2_avg\n",
    "df_word2vec['cleaned_question1'] = clean_sentence_1\n",
    "df_word2vec['cleaned_question2'] = clean_sentence_2\n",
    "df_word2vec['is_duplicate'] = is_duplicate\n",
    "df_word2vec['cos_sum'] = cos_sum\n",
    "df_word2vec['cos_avg'] = cos_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the embedding into a csv\n",
    "df_word2vec.to_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type2/data/Word2Vec/test_word2vec.csv\", index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word2vec.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
