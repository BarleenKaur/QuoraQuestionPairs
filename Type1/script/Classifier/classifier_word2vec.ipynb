{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import types\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from sklearn import svm\n",
    "import codecs, csv, sys\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB                                                                \n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the csv containg embedding for test and train embedding data\n",
    "df_train = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type1/data/Word2Vec/train_word2vec.csv\")\n",
    "df_test = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type1/data/Word2Vec/test_word2vec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenating the embedding( sum and average) for question1 and question 2 and normalizing before passing it to classifiers\n",
    "start_time = time.time()\n",
    "\n",
    "train_embedding1_sum = map(np.fromstring, df_train['embedding_1_sum'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "\n",
    "train_embedding2_sum = map(np.fromstring, df_train['embedding_2_sum'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "\n",
    "train_embedding1_avg = map(np.fromstring, df_train['embedding_1_avg'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "\n",
    "train_embedding2_avg = map(np.fromstring, df_train['embedding_2_avg'],\\\n",
    "                                      itertools.repeat(float, df_train.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_train.shape[0]))\n",
    "X_label = np.array(df_train['is_duplicate'].tolist())\n",
    "X_sum = np.concatenate((np.array(train_embedding1_sum), np.array(train_embedding2_sum)), axis=1)\n",
    "normalize_sum = Normalizer().fit(X_sum)\n",
    "X_sum = normalize_sum.transform(X_sum)\n",
    "\n",
    "X_avg = np.concatenate((np.array(train_embedding1_avg), np.array(train_embedding2_avg)), axis=1)\n",
    "normalize_avg = Normalizer().fit(X_avg)\n",
    "X_avg = normalize_avg.transform(X_avg)\n",
    "\n",
    "test_embedding1_sum = map(np.fromstring, df_test['embedding_1_sum'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "\n",
    "test_embedding2_sum = map(np.fromstring, df_test['embedding_2_sum'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "\n",
    "test_embedding1_avg = map(np.fromstring, df_test['embedding_1_avg'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "\n",
    "test_embedding2_avg = map(np.fromstring, df_test['embedding_2_avg'],\\\n",
    "                                      itertools.repeat(float, df_test.shape[0]),\\\n",
    "                                                       itertools.repeat(300, df_test.shape[0]))\n",
    "Y_label = np.array(df_test['is_duplicate'].tolist())\n",
    "Y_sum = np.concatenate((np.array(test_embedding1_sum), np.array(test_embedding2_sum)), axis=1) # concatenating the embedding for question 1 and question2\n",
    "Y_sum = normalize_sum.transform(Y_sum)\n",
    "\n",
    "Y_avg = np.concatenate((np.array(test_embedding1_sum), np.array(test_embedding2_sum)), axis=1)\n",
    "Y_avg = normalize_avg.transform(Y_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting is_duplicate value by passing embedding into shallow classifiers\n",
    "def naive_bayes(x_train_matrix, x_test_matrix, y_train, y_test):    #NAIVE BAYERS MODEL                                                                \n",
    "    nb = MultinomialNB(alpha=2.8)\n",
    "    nb.fit(x_train_matrix, y_train)                                 #train the model\n",
    "    y_pred = nb.predict(x_test_matrix)                              #make predictions for X_test\n",
    "    print (\"Naive Bayes Model score: \" + str(accuracy_score(y_test, y_pred)))         \n",
    "    print (\"Naive Bayes Model confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred))\n",
    "    y_predicted_proba = nb.predict_proba(x_test_matrix)\n",
    "    print log_loss(y_test, y_predicted_proba)\n",
    "    return y_pred\n",
    "\n",
    "def logistic_regression(x_train_matrix, x_test_matrix, y_train, y_test): \n",
    "    logReg = LogisticRegressionCV(Cs=[0.01, 0.1, 1, 10, 100], cv=5, solver='saga', n_jobs= -1 )\n",
    "    logReg.fit(x_train_matrix, y_train)\n",
    "    y_pred_log = logReg.predict(x_test_matrix)\n",
    "    print (\"Logistic Regression score: \"+ str(accuracy_score(y_test, y_pred_log)))\n",
    "    print (\"Logistic Regression confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred_log))\n",
    "    y_predicted_proba = logReg.predict_proba(x_test_matrix)\n",
    "    print log_loss(y_test, y_predicted_proba)\n",
    "    return y_pred_log\n",
    "\n",
    "def linear_svm(x_train_matrix, x_test_matrix, y_train, y_test):\n",
    "    \n",
    "    param_grid = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']}]\n",
    "    svc = svm.SVC()\n",
    "    svm1 = GridSearchCV(estimator=svc, param_grid=param_grid, n_jobs= -1, cv = 5) \n",
    "    #svm1 = LinearSVC(C=1)\n",
    "    svm1.fit(x_train_matrix, y_train)\n",
    "    y_pred_svc = svm1.predict(x_test_matrix)\n",
    "    print (\"SVC score: \" + str(accuracy_score(y_test, y_pred_svc)))\n",
    "    print (\"SVC confusionmatrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred_svc))\n",
    "    return y_pred_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_time = time.time()\n",
    "print(\"--- start -> preprocessing:%s seconds ---\" % (preprocessing_time - start_time))\n",
    "\n",
    "\n",
    "print (\"\\n\\n Naive bayes result on word_embedding_sum: \")\n",
    "y_pred_nb_sum = naive_bayes(X_sum, Y_sum, X_label, Y_label)\n",
    "print (\"\\n\\n Naive bayes result on word_embedding_avg: \")\n",
    "y_pred_nb_avg = naive_bayes(X_avg, Y_avg, X_label, Y_label)\n",
    "naive_bayes_time = time.time() \n",
    "print(\"--- preprocessing -> naive bayes :%s seconds ---\" % (naive_bayes_time - preprocessing_time))\n",
    "\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "print (\"\\n\\n Logistic Regression result on word_embedding_sum: \")\n",
    "y_pred_lr_sum = logistic_regression(X_sum, Y_sum, X_label, Y_label)\n",
    "print (\"\\n\\n Logistic Regression result on word_embedding_avg: \")\n",
    "y_pred_lr_avg = logistic_regression(X_avg, Y_avg, X_label, Y_label)\n",
    "logistic_regression_time = time.time()\n",
    "print(\"--- naive bayes -> logistic regression :%s seconds ---\" % (logistic_regression_time - naive_bayes_time))\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "print (\"\\n\\n Linear SVM result on word_embedding_sum: \")\n",
    "y_pred_svm_sum = linear_svm(X_sum, Y_sum, X_label, Y_label)\n",
    "print (\"\\n\\n Linear SVM result on word_embedding_avg: \")\n",
    "y_pred_svm_avg = linear_svm(X_avg, Y_avg, X_label, Y_label)\n",
    "linear_svm_time = time.time()\n",
    "print(\"--- logistic regression -> linear svm :%s seconds ---\" % (linear_svm_time - logistic_regression_time))\n",
    "print(\"--- TOTAL TIME: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_nb_sum = y_pred_nb_sum.tolist()\n",
    "y_pred_nb_avg = y_pred_nb_avg.tolist()\n",
    "y_pred_lr_sum = y_pred_lr_sum.tolist()\n",
    "y_pred_lr_avg = y_pred_lr_avg.tolist()\n",
    "y_pred_svm_sum = y_pred_svm_sum.tolist()\n",
    "y_pred_svm_avg = y_pred_svm_avg.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['nb_sum_duplicate'] = y_pred_nb_sum\n",
    "df_test['nb_avg_duplicate'] = y_pred_nb_avg\n",
    "df_test['lr_sum_duplicate'] = y_pred_lr_sum\n",
    "df_test['lr_avg_duplicate'] = y_pred_lr_avg\n",
    "df_test['svm_sum_duplicate'] = y_pred_svm_sum\n",
    "df_test['svm_avg_duplicate'] = y_pred_svm_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.to_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type1/Results/test_word2vec_classifier.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparing the cosine similarity between word2vec vectors on different thresholds\n",
    "df_new = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type1/Results/test_word2vec_classifier.csv\")\n",
    "\n",
    "def conv(elem):\n",
    "    a = re.compile(r\"[-+]?\\d*\\.\\d+|\\d+\")\n",
    "    return re.findall(a, elem)[0]\n",
    "\n",
    "threshold_list = [0.6, 0.7,0.78, 0.8]\n",
    "df_cos_test = df_new\n",
    "df_cos_test['cos_avg'] = map(conv, df_cos_test['cos_avg'])\n",
    "df_cos_test['cos_sum'] = map(conv, df_cos_test['cos_sum'])\n",
    "df_cos_test[['cos_avg','cos_sum']] = df_cos_test[['cos_avg','cos_sum']].apply(pd.to_numeric)\n",
    "\n",
    "for threshold in threshold_list:\n",
    "    \n",
    "    print \"Threshold: \", threshold\n",
    "    \n",
    "    df_cos_test['thre_sum_'+str(threshold)] = np.where(df_cos_test['cos_sum']>=threshold, 1, 0)\n",
    "    df_cos_test['thre_avg_'+str(threshold)] = np.where(df_cos_test['cos_avg']>=threshold, 1, 0)\n",
    "\n",
    "    print confusion_matrix(df_cos_test['is_duplicate'], df_cos_test['thre_sum_'+str(threshold)]\\\n",
    "                                      , labels=target_names).ravel()\n",
    "    print classification_report(df_cos_test['is_duplicate'], df_cos_test['thre_sum_'+str(threshold)])\n",
    "    print accuracy_score(df_cos_test['is_duplicate'], df_cos_test['thre_sum_'+str(threshold)])\n",
    "    print confusion_matrix(df_cos_test['is_duplicate'], df_cos_test['thre_avg_'+str(threshold)]\\\n",
    "                                      , labels=target_names).ravel()\n",
    "    \n",
    "    print classification_report(df_cos_test['is_duplicate'], df_cos_test['thre_avg_'+str(threshold)])\n",
    "\n",
    "    print \"Accuracy: \" ,accuracy_score(df_cos_test['is_duplicate'], df_cos_test['thre_avg_'+str(threshold)])\n",
    "    print \"\\n*******************\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
