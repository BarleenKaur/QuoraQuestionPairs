{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import gensim\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, log_loss_loss\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the train dataset\n",
    "df_train = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type1/data/Preprocessed/preprocessed_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "sentence_1 = df_train['question1']\n",
    "sentence_2 = df_train['question2']\n",
    "clean_sentence_1 = df_train['cleaned_question1']\n",
    "clean_sentence_2 = df_train['cleaned_question2']\n",
    "is_duplicate = df_train['is_duplicate']\n",
    "id_ = df_train['index']\n",
    "qid1 = df_train['qid1']\n",
    "qid2 = df_train['qid2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training the doc2vec model using Quora training data\n",
    "taggeddocs = []\n",
    "tag2questionmap = {} \n",
    "\n",
    "for c_id, id1 , id2 , q1 , q2, label in zip(id_, qid1, qid2 , clean_sentence_1, clean_sentence_2, is_duplicate):\n",
    "    questions = [q1, q2]\n",
    "    try:\n",
    "        for index,i in enumerate(questions):\n",
    "            #if len(i) > 2 : # Non empty tweets\n",
    "            #print i\n",
    "            if index == 0: \n",
    "                tag = u'SENT_{:d}'.format(id1)\n",
    "            else:\n",
    "                tag = u'SENT_{:d}'.format(id2)\n",
    "            #print i\n",
    "            sentence = TaggedDocument(words=gensim.utils.to_unicode(i).split(), tags=[tag])\n",
    "            #print sentence\n",
    "            tag2questionmap[tag] = i\n",
    "            taggeddocs.append(sentence)       \n",
    "    except:\n",
    "        print \"c_id: \", c_id, \"q1: \" , q1, \"q2: \", q2\n",
    "   \n",
    "    \n",
    "\n",
    "#model = gensim.models.Doc2Vec(taggeddocs, dm=0, alpha=0.025, size=100, min_alpha=0.025, min_count=2)\n",
    "model = gensim.models.doc2vec.Doc2Vec(size=100, min_count=1, alpha=0.1)\n",
    "model.build_vocab(taggeddocs)\n",
    "%time model.train(taggeddocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting the embeddings for dataset from the trained model\n",
    "embedding_1 = []\n",
    "embedding_2 = []\n",
    "cos = []\n",
    "for c_id, q1 , q2 in zip(id_, clean_sentence_1, clean_sentence_2):  \n",
    "    questions = [q1, q2]\n",
    "    embeddings = []\n",
    "\n",
    "    for index, question in enumerate(questions): \n",
    "        if question != question:   #checking for Nan values\n",
    "            print \"c_id: \", c_id\n",
    "            doc2vec_embed = np.random.uniform(low=-1.0, high=1.0, size=(100,)).reshape(1,-1)\n",
    "            if index == 0:\n",
    "                embedding_1.append(doc2vec_embed[0])\n",
    "            else:\n",
    "                embedding_2.append(doc2vec_embed[0])\n",
    "        \n",
    "        else:\n",
    "            question_token = tokenizer.tokenize(question)\n",
    "            doc2vec_embed = model.infer_vector(question_token).reshape(1,-1)\n",
    "            if index == 0:\n",
    "                embedding_1.append(doc2vec_embed[0])\n",
    "            else:\n",
    "                embedding_2.append(doc2vec_embed[0])\n",
    "        embeddings.append(doc2vec_embed)\n",
    "            \n",
    "    cos.append(cosine_similarity(embeddings[0],embeddings[1])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_doc2vec = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_doc2vec['id'] = id_\n",
    "df_doc2vec['qid1'] = qid1\n",
    "df_doc2vec['qid2'] = qid2\n",
    "df_doc2vec['embedding_1'] = embedding_1\n",
    "df_doc2vec['embedding_2'] = embedding_2\n",
    "df_doc2vec['cleaned_question1'] = clean_sentence_1\n",
    "df_doc2vec['cleaned_question2'] = clean_sentence_1\n",
    "df_doc2vec['is_duplicate'] = is_duplicate\n",
    "df_doc2vec['cos'] = cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the embedding into a csv\n",
    "df_doc2vec.to_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type1/data/doc2Vec/train_doc2vec.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
