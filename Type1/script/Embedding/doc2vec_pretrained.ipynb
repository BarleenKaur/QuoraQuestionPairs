{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import gensim.models.doc2vec\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import gensim\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the train dataset\n",
    "df_train = pd.read_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type1/data/Preprocessed/preprocessed_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "sentence_1 = df_train['question1']\n",
    "sentence_2 = df_train['question2']\n",
    "clean_sentence_1 = df_train['cleaned_question1']\n",
    "clean_sentence_2 = df_train['cleaned_question2']\n",
    "is_duplicate = df_train['is_duplicate']\n",
    "id_ = df_train['index']\n",
    "qid1 = df_train['qid1']\n",
    "qid2 = df_train['qid2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the pre-trained doc2vec model\n",
    "doc2vec_model = gensim.models.Doc2Vec.load(\"/Volumes/Barly/NLP/Project/enwiki_dbow/doc2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#python example to infer document vectors from trained doc2vec model\n",
    "\n",
    "embedding_1 = []\n",
    "embedding_2 = []\n",
    "cos = []\n",
    "for c_id, q1 , q2 in zip(id_, clean_sentence_1, clean_sentence_2):  \n",
    "    questions = [q1, q2]\n",
    "    embeddings = []\n",
    "\n",
    "    for index, question in enumerate(questions): \n",
    "        if question != question:   #checking for Nan values\n",
    "            print \"c_id: \", c_id\n",
    "            doc2vec_embed = np.random.uniform(low=-0.2, high=0.2, size=(300,)).reshape(1,-1)\n",
    "            if index == 0:\n",
    "                embedding_1.append(doc2vec_embed[0])\n",
    "            else:\n",
    "                embedding_2.append(doc2vec_embed[0])\n",
    "        \n",
    "        else:\n",
    "            question_token = tokenizer.tokenize(question)\n",
    "            doc2vec_embed = doc2vec_model.infer_vector(question_token,alpha=0.01, steps=1000).reshape(1,-1)\n",
    "            if index == 0:\n",
    "                embedding_1.append(doc2vec_embed[0])\n",
    "            else:\n",
    "                embedding_2.append(doc2vec_embed[0])\n",
    "        embeddings.append(doc2vec_embed)\n",
    "            \n",
    "    cos.append(cosine_similarity(embeddings[0],embeddings[1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_doc2vec = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_doc2vec['id'] = id_\n",
    "df_doc2vec['qid1'] = qid1\n",
    "df_doc2vec['qid2'] = qid2\n",
    "df_doc2vec['embedding_1'] = embedding_1\n",
    "df_doc2vec['embedding_2'] = embedding_2\n",
    "df_doc2vec['cleaned_question1'] = clean_sentence_1\n",
    "df_doc2vec['cleaned_question2'] = clean_sentence_2\n",
    "df_doc2vec['is_duplicate'] = is_duplicate\n",
    "df_doc2vec['cos'] = cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the embeddings into a csv\n",
    "df_doc2vec.to_csv(\"/Volumes/Barly/NLP/Project/5k_testing/Type1/data/doc2Vec/train_doc2vec_pretrained.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
