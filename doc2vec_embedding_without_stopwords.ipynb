{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Barly/Anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "from autocorrect import spell\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import gensim\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB                                                                \n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_1a = df_train['question1']\n",
    "sentence_1b = df_train['question2']\n",
    "is_duplicate = df_train['is_duplicate']\n",
    "id_ = df_train['id']\n",
    "qid1 = df_train['qid1']\n",
    "qid2 = df_train['qid2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "punctuation_list = list(string.punctuation)\n",
    "\n",
    "def spelling_correction(question):\n",
    "    question_updated = \"\"\n",
    "    #for token in word_tokenize(question):\n",
    "    for token in tokenizer.tokenize(question):\n",
    "        word = ''.join(ch for ch in token if ch not in punctuation_list and ch.isalnum())\n",
    "        question_updated =  question_updated + word + \" \"\n",
    "    return question_updated\n",
    "\n",
    "def remove_stopword_questions(question):\n",
    "    temp_list = [token for token in tokenizer.tokenize(question) if token not in stopwords.words('english')]\n",
    "    return ' '.join(temp_list)\n",
    "\n",
    "def get_preprocessed_tokens(question):  \n",
    "    question = spelling_correction(question)\n",
    "    question = question.lower()\n",
    "    question = remove_stopword_questions(question)\n",
    "    question = [word for word in tokenizer.tokenize(question)]\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(question):\n",
    "    joined_tokens = []\n",
    "    lemma_question = \"\"\n",
    "    for token in get_preprocessed_tokens(question):\n",
    "        token_lemma = wordnet_lemmatizer.lemmatize(token)\n",
    "        joined_tokens.append(token_lemma)\n",
    "    return joined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105780\n",
      "201841\n",
      "CPU times: user 47min 52s, sys: 25min 32s, total: 1h 13min 25s\n",
      "Wall time: 48min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "510982312"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggeddocs = []\n",
    "tag2questionmap = {} \n",
    "df_doc2vec = pd.DataFrame()\n",
    "for c_id, id1 , id2 , q1 , q2, label in zip(id_, qid1, qid2 , sentence_1a, sentence_1b, is_duplicate):\n",
    "    questions = [q1, q2]\n",
    "    try:\n",
    "        cleaned_questions = []\n",
    "        for question in questions:\n",
    "            words = lemmatize_tokens(question)\n",
    "            cleaned_question = \" \".join(w for w in words)\n",
    "            cleaned_questions.append(cleaned_question)\n",
    "        df_doc2vec_temp = pd.DataFrame({'index':[c_id], 'cleaned_question1': [cleaned_questions[0]], 'cleaned_question2': [cleaned_questions[1]],\\\n",
    "                                        'is_duplicate': [label],'question1': [q1], 'question2': [q2]})\n",
    "        df_doc2vec = pd.concat([df_doc2vec, df_doc2vec_temp])\n",
    "        for index,i in enumerate(cleaned_questions):\n",
    "            #if len(i) > 2 : # Non empty tweets\n",
    "            #print i\n",
    "            if index == 0: \n",
    "                tag = u'SENT_{:d}'.format(id1)\n",
    "            else:\n",
    "                tag = u'SENT_{:d}'.format(id2)\n",
    "            sentence = TaggedDocument(words=gensim.utils.to_unicode(i).split(), tags=[tag])\n",
    "            #print sentence\n",
    "            tag2questionmap[tag] = i\n",
    "            taggeddocs.append(sentence)       \n",
    "    except:\n",
    "        print c_id\n",
    "    \n",
    "    \n",
    "\n",
    "#model = gensim.models.Doc2Vec(taggeddocs, dm=0, alpha=0.025, size=100, min_alpha=0.025, min_count=2)\n",
    "model = gensim.models.doc2vec.Doc2Vec(size=100, min_count=2, alpha=0.1)\n",
    "model.build_vocab(taggeddocs)\n",
    "%time model.train(taggeddocs, total_examples=model.corpus_count, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cleaned_question1 = df_doc2vec['cleaned_question1']\n",
    "cleaned_question2 = df_doc2vec['cleaned_question2']\n",
    "duplicated_tag = df_doc2vec['is_duplicate']\n",
    "df_doc2vec_org = df_doc2vec\n",
    "\n",
    "embedding_1 = []\n",
    "embedding_2 = []\n",
    "cos = []\n",
    "for  q1 , q2, label in zip(cleaned_question1, cleaned_question2, duplicated_tag):  \n",
    "    questions = [q1, q2]\n",
    "    embeddings = []\n",
    "    for index, question in enumerate(questions): \n",
    "        question_token = tokenizer.tokenize(question)\n",
    "        doc2vec_embed = model.infer_vector(question_token).reshape(1,-1)\n",
    "        if index == 0:\n",
    "            embedding_1.append(doc2vec_embed[0])\n",
    "        else:\n",
    "            embedding_2.append(doc2vec_embed[0])\n",
    "        embeddings.append(doc2vec_embed)\n",
    "    cos.append(cosine_similarity(embeddings[0],embeddings[1])[0])\n",
    "    \n",
    "df_doc2vec['embedding_1'] = embedding_1\n",
    "df_doc2vec['embedding_2'] = embedding_2\n",
    "df_doc2vec['cos'] = cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_doc2vec.to_csv(\"doc2vec_embeddings_without_stopwords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[196725  58300]\n",
      " [ 84406  64857]]\n",
      "Accuracy:  0.647018956783\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.77      0.73    255025\n",
      "          1       0.53      0.43      0.48    149263\n",
      "\n",
      "avg / total       0.64      0.65      0.64    404288\n",
      "\n",
      "[[226296  28729]\n",
      " [115133  34130]]\n",
      "Accuracy:  0.644159608992\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.89      0.76    255025\n",
      "          1       0.54      0.23      0.32    149263\n",
      "\n",
      "avg / total       0.62      0.64      0.60    404288\n",
      "\n",
      "[[246387   8638]\n",
      " [138971  10292]]\n",
      "Accuracy:  0.634891463511\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.97      0.77    255025\n",
      "          1       0.54      0.07      0.12    149263\n",
      "\n",
      "avg / total       0.60      0.63      0.53    404288\n",
      "\n",
      "[[254007   1018]\n",
      " [148125   1138]]\n",
      "Accuracy:  0.631097138673\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      1.00      0.77    255025\n",
      "          1       0.53      0.01      0.02    149263\n",
      "\n",
      "avg / total       0.59      0.63      0.49    404288\n",
      "\n",
      "[[254986     39]\n",
      " [149214     49]]\n",
      "Accuracy:  0.630825055406\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      1.00      0.77    255025\n",
      "          1       0.56      0.00      0.00    149263\n",
      "\n",
      "avg / total       0.60      0.63      0.49    404288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold_list = [0.5 , 0.6 , 0.7, 0.8, 0.9]\n",
    "cosine = df_doc2vec['cos']\n",
    "y_true = duplicated_tag\n",
    "for threshold in threshold_list:\n",
    "    is_dup_pred = []\n",
    "    for cos in cosine:\n",
    "        if cos > threshold:\n",
    "            is_dup_pred.append(1)\n",
    "        else:\n",
    "            is_dup_pred.append(0)\n",
    "        \n",
    "    df_doc2vec['is_dup_pred'+ str(threshold)] = is_dup_pred\n",
    "    y_pred = is_dup_pred\n",
    "    target_names = [0, 1]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=target_names).ravel()\n",
    "    print confusion_matrix(y_true, y_pred, labels=target_names)\n",
    "    accuracy = float(tn + tp)/(tn + fp + fn + tp)\n",
    "    print \"Accuracy: \", accuracy\n",
    "    print classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding1 = df_doc2vec['embedding_1']\n",
    "embedding2 = df_doc2vec['embedding_2']\n",
    "train_label = duplicated_tag\n",
    "train_questions = embedding1 + embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayer(x_train_matrix, x_test_matrix, y_train, y_test):    #NAIVE BAYERS MODEL\n",
    "                                                                    \n",
    "    nb = GaussianNB()\n",
    "    nb.fit(x_train_matrix, y_train)                                 #train the model\n",
    "    y_pred = nb.predict(x_test_matrix)                              #make predictions for X_test\n",
    "    print (\"Naive Byers Model score: \" + str(accuracy_score(y_test, y_pred)))         \n",
    "    print (\"Naive Byers Model confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def logistic_regression(x_train_matrix, x_test_matrix, y_train, y_test): \n",
    "    logReg = LogisticRegression(C=0.85)\n",
    "    logReg.fit(x_train_matrix, y_train)\n",
    "    y_pred_log = logReg.predict(x_test_matrix)\n",
    "    print (\"Logistic Regression score: \"+ str(accuracy_score(y_test, y_pred_log)))\n",
    "    print (\"Logistic Regression confusion matrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred_log))\n",
    "\n",
    "def linear_svm(x_train_matrix, x_test_matrix, y_train, y_test):\n",
    "    svm1 = LinearSVC(C=1)\n",
    "    svm1.fit(x_train_matrix, y_train)\n",
    "    y_pred_svc = svm1.predict(x_test_matrix)\n",
    "    print (\"SVC score: \" + str(accuracy_score(y_test, y_pred_svc)))\n",
    "    print (\"SVC confusionmatrix:\")\n",
    "    print (confusion_matrix(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- start -> preprocessing:0.319641113281 seconds ---\n",
      "Naive Byers Model score: 0.561480924489\n",
      "Naive Byers Model confusion matrix:\n",
      "[[31824 31940]\n",
      " [12382 24926]]\n",
      "--- preprocessing -> naive bayer :1.08264183998 seconds ---\n",
      "-------------------------------------------\n",
      "Logistic Regression score: 0.634824679436\n",
      "Logistic Regression confusion matrix:\n",
      "[[59076  4688]\n",
      " [32221  5087]]\n",
      "--- niave bayer -> logistic regression :3.42052698135 seconds ---\n",
      "-------------------------------------------\n",
      "SVC score: 0.63494340668\n",
      "SVC confusionmatrix:\n",
      "[[59574  4190]\n",
      " [32707  4601]]\n",
      "--- logistic regression -> linear svm :176.277682066 seconds ---\n",
      "--- TOTAL TIME: 181.100629091 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "train_question_label = zip(train_questions,train_label) \n",
    "train_df = pd.DataFrame(train_question_label, columns=['embedding', 'label'])\n",
    "X = train_df.embedding\n",
    "Y = train_df.label\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state=1)\n",
    "\n",
    "preprocessing_time = time.time()\n",
    "print(\"--- start -> preprocessing:%s seconds ---\" % (preprocessing_time - start_time))\n",
    "\n",
    "X_train = np.array(X_train.tolist())\n",
    "X_test = np.array(X_test.tolist())\n",
    "naive_bayer(X_train, X_test, Y_train, Y_test)\n",
    "naive_bayer_time = time.time() \n",
    "print(\"--- preprocessing -> naive bayer :%s seconds ---\" % (naive_bayer_time - preprocessing_time))\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "\n",
    "logistic_regression(X_train, X_test, Y_train, Y_test)\n",
    "logistic_regression_time = time.time()\n",
    "print(\"--- niave bayer -> logistic regression :%s seconds ---\" % (logistic_regression_time - naive_bayer_time))\n",
    "\n",
    "print (\"-------------------------------------------\")\n",
    "\n",
    "linear_svm(X_train, X_test, Y_train, Y_test)\n",
    "linear_svm_time = time.time()\n",
    "print(\"--- logistic regression -> linear svm :%s seconds ---\" % (linear_svm_time - logistic_regression_time))\n",
    "print(\"--- TOTAL TIME: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
